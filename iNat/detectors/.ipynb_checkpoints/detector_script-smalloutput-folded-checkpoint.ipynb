{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "\n",
    "from IPython import display\n",
    "from copy import deepcopy\n",
    "from skimage.transform import resize\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Important Values\n",
    "\n",
    "data = '/data/dww78/mini_inat_val/'\n",
    "gpu = 0\n",
    "torch.cuda.set_device(gpu)\n",
    "workers = 8\n",
    "epoch = 10\n",
    "start_epoch = 0\n",
    "vbity = 50\n",
    "esize = 2\n",
    "\n",
    "way = 20\n",
    "evalway = 5\n",
    "trainshot = 5\n",
    "testshot = 15\n",
    "foldshot = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n",
      "681 227\n"
     ]
    }
   ],
   "source": [
    "# Load Training/Testing Data\n",
    "threshold = .15\n",
    "d_train = torch.load(data+'train_boxes.pth')\n",
    "d_test = torch.load(data+'val_boxes.pth')\n",
    "d_boxes = torch.load(data+'box_coords.pth')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4905, 0.4961, 0.4330],std=[0.1737, 0.1713, 0.1779])\n",
    "    ])\n",
    "\n",
    "def load_transform(path, pathdict, boxdict, transform, size, msize, flipping, masking):\n",
    "    flip = np.random.choice([True, False])\n",
    "    with open(path, 'rb') as f:\n",
    "        p = Image.open(f)\n",
    "        p = p.convert('RGB')\n",
    "    w,h = p.size\n",
    "    p = p.resize((size, size), Image.BILINEAR)\n",
    "    if flip and flipping:\n",
    "        p = p.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    t = transform(p)\n",
    "    if masking:\n",
    "        mask = np.zeros((h,w))\n",
    "        boxes = pathdict[path]\n",
    "        for b in boxes:\n",
    "            box = boxdict[b]\n",
    "            xmin = box[0]\n",
    "            xmax = box[2]+xmin\n",
    "            ymin = box[1]\n",
    "            ymax = box[3]+ymin\n",
    "            if not flip or not flipping:\n",
    "                mask[ymin:ymax, xmin:xmax] = 1\n",
    "            else:\n",
    "                mask[ymin:ymax, w-xmax:w-xmin] = 1\n",
    "        mask = resize(mask, (msize,msize), mode='constant', cval=0, anti_aliasing=False)\n",
    "        t = [t, (torch.FloatTensor(mask-threshold).sign()/2+.5).unsqueeze(0)]\n",
    "    return t\n",
    "\n",
    "class FoldSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, data_source, way):\n",
    "        iddict = dict()\n",
    "        for i,(_,cat) in enumerate(data_source.imgs):\n",
    "            if cat in iddict:\n",
    "                iddict[cat].append(i)\n",
    "            else:\n",
    "                iddict[cat] = [i]\n",
    "        self.iddict = iddict\n",
    "        self.way = way\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Build new dictionary, shuffle entries\n",
    "        trackdict = deepcopy(self.iddict)\n",
    "        for key in trackdict:\n",
    "            np.random.shuffle(trackdict[key])\n",
    "        # Choose categories, sample, eliminate small categories\n",
    "        idlist = []\n",
    "        while len(trackdict.keys()) >= self.way:\n",
    "            cats = np.random.choice(list(trackdict.keys()), size=self.way, replace=False)\n",
    "            for cat in cats:\n",
    "                for _ in range(foldshot):\n",
    "                    idlist.append(trackdict[cat].pop())\n",
    "                if len(trackdict[cat]) < foldshot:\n",
    "                    trackdict.pop(cat)\n",
    "            # TODO: shuffle idlist batches\n",
    "            yield idlist\n",
    "            idlist = []\n",
    "\n",
    "class ProtoSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, data_source, way):\n",
    "        iddict = dict()\n",
    "        for i,(_,cat) in enumerate(data_source.imgs):\n",
    "            if cat in iddict:\n",
    "                iddict[cat].append(i)\n",
    "            else:\n",
    "                iddict[cat] = [i]\n",
    "        self.iddict = iddict\n",
    "        self.way = way\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Build new dictionary, shuffle entries\n",
    "        trackdict = deepcopy(self.iddict)\n",
    "        for key in trackdict:\n",
    "            np.random.shuffle(trackdict[key])\n",
    "        # Choose categories, sample, eliminate small categories\n",
    "        idlist = []\n",
    "        while len(trackdict.keys()) >= self.way:\n",
    "            cats = np.random.choice(list(trackdict.keys()), size=self.way, replace=False)\n",
    "            for cat in cats:\n",
    "                for _ in range(trainshot):\n",
    "                    idlist.append(trackdict[cat].pop())\n",
    "            for cat in cats:\n",
    "                for _ in range(testshot):\n",
    "                    idlist.append(trackdict[cat].pop())\n",
    "                if len(trackdict[cat]) < trainshot+testshot:\n",
    "                    trackdict.pop(cat)\n",
    "            # TODO: shuffle idlist batches\n",
    "            yield idlist\n",
    "            idlist = []\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    data+'train', \n",
    "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, 21, True, True))\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    data+'val',\n",
    "    loader = lambda x: load_transform(x, d_test, d_boxes, transform, 84, 21, False, True))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_sampler = FoldSampler(train_dataset, way),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_sampler = ProtoSampler(test_dataset, evalway),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "print('Data loaded!')\n",
    "print(len(train_dataset.classes), len(test_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113088  parameters in each neural net.\n"
     ]
    }
   ],
   "source": [
    "# Make Models\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, insize, outsize):\n",
    "        super(Block, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(insize, outsize, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(outsize)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return self.layers(inp)\n",
    "\n",
    "class PROTO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PROTO, self).__init__()\n",
    "        self.process = nn.Sequential(\n",
    "            Block(3,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            Block(64,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            Block(64,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Block(64,64)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return self.process(inp)\n",
    "        \n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.sm = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, inp, masks, way, shot):\n",
    "        if self.training:\n",
    "            support = inp.unsqueeze(-1) # B 64 21 21 1\n",
    "            m = masks.unsqueeze(1) # B 1 21 21\n",
    "            c = torch.stack([1-m, m], dim=-1) # B 1 21 21 2     (Positive class corresponds to index/label 1)\n",
    "            c = (c*support).contiguous().view(way,shot,inp.size(1),-1,2).mean(3) # W S 64 2         (Each image's prototypes)\n",
    "            c = (c.sum(1).unsqueeze(1) - c)/(shot-1) # W S 64 2        (Rescaled folded prototypes)\n",
    "            c = c.unsqueeze(3).unsqueeze(3) # W S 64 1 1 2\n",
    "\n",
    "            query = inp.view(way, -1, inp.size(1), inp.size(2), inp.size(3), 1) # W S 64 21 21 1\n",
    "        else:\n",
    "            support = inp[:way*shot].unsqueeze(-1) # B 64 21 21 1\n",
    "            m = masks.unsqueeze(1) # B 1 21 21\n",
    "            c = torch.stack([1-m, m], dim=-1) # B 1 21 21 2     (Positive class corresponds to index/label 1)\n",
    "            c = (c*support).contiguous().view(way,shot,inp.size(1),-1,2).mean(3).mean(1) # W 64 2\n",
    "            c = c.unsqueeze(2).unsqueeze(2).unsqueeze(1) # W 1 64 1 1 2\n",
    "\n",
    "            query = inp[way*trainshot:].view(way, -1, inp.size(1), inp.size(2), inp.size(3), 1) # W S 64 21 21 1\n",
    "        \n",
    "        distmat = ((c-query)**2).sum(2).neg().view(-1, inp.size(2), inp.size(3), 2) # B 21 21 2\n",
    "        probs = self.sm(distmat)\n",
    "        return probs\n",
    "    \n",
    "smodel = [PROTO().cuda() for i in range(esize)]\n",
    "predictor = Predictor().cuda()\n",
    "soptimizer = [optim.Adam(m.parameters(), lr=.001) for m in smodel]\n",
    "sscheduler = [optim.lr_scheduler.LambdaLR(o, lambda x: 1/(2**x)) for o in soptimizer]\n",
    "criterion = nn.NLLLoss().cuda()\n",
    "\n",
    "nweights = sum([i.numel() for i in list(smodel[0].parameters())])\n",
    "print(nweights,\" parameters in each neural net.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Define the Procedures\n",
    "\n",
    "def train(train_loader, epoch, gpu, vbity):\n",
    "    for model in smodel:\n",
    "        model.train()\n",
    "    predictor.train()\n",
    "    allloss = [0]*esize\n",
    "    for i, ((inp,m), _) in enumerate(train_loader):\n",
    "        im = Variable(inp).cuda(device = gpu, async=True) # B 3 84 84\n",
    "        m = Variable(m).cuda(device = gpu, async=True) # B 1 21 21\n",
    "        mask = m \n",
    "        targ = m.long() \n",
    "        \n",
    "        for j in range(esize):\n",
    "            smodel[j].zero_grad()\n",
    "            out = predictor(smodel[j](im), mask, way, foldshot)\n",
    "            loss = criterion(out.view(-1,2), targ.view(-1))\n",
    "            loss.backward()\n",
    "            soptimizer[j].step()\n",
    "        \n",
    "            allloss[j] += loss.item()\n",
    "            \n",
    "        if i%vbity == 0:\n",
    "            print('%d of approx. 192270'%(i*way*foldshot))\n",
    "    return [L/i for L in allloss]\n",
    "\n",
    "def validate(val_loader, gpu, vbity, reps, verbose):\n",
    "    for model in smodel:\n",
    "        model.eval()\n",
    "    predictor.eval()\n",
    "    targ = Variable(torch.LongTensor([i//testshot for i in range(testshot*evalway)])).cuda(\n",
    "        device = gpu, async=True)\n",
    "    allloss = [0]*esize\n",
    "    acctracker = [[] for _ in range(esize)]\n",
    "    for r in range(reps):\n",
    "        for i, ((inp,m), _) in enumerate(val_loader):\n",
    "            im = Variable(inp).cuda(device = gpu, async=True) # B 3 84 84\n",
    "            m = Variable(m).cuda(device = gpu, async=True) # B 1 21 21\n",
    "            mask = m[:evalway*trainshot] \n",
    "            targ = m[evalway*trainshot:].long() \n",
    "\n",
    "            with torch.no_grad():\n",
    "                for j in range(esize):\n",
    "                    out = predictor(smodel[j](im), mask, evalway, trainshot)\n",
    "                    loss = criterion(out.view(-1,2), targ.view(-1))\n",
    "                    allloss[j] += loss.item()\n",
    "                    _,pred = torch.max(out,-1)\n",
    "                    intersection = (pred*targ).sum().item()\n",
    "                    union = (1-(1-targ)*(1-pred)).sum().item()\n",
    "                    acc = intersection/union\n",
    "                    acctracker[j].append(acc)\n",
    "            if i%vbity == 0 and verbose:\n",
    "                print('Round %d of %d, %d of approx. 51716'%(r+1, reps, i*evalway*(trainshot+testshot)))\n",
    "    return [L/i for L in allloss], [np.mean(a) for a in acctracker], [1.96*np.sqrt(np.var(a)/len(a)) for a in acctracker]\n",
    "\n",
    "print('Ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-64a8c01d28c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrainloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvbity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Evaluate, single pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e70dcc7adae8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, epoch, gpu, vbity)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mesize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0msmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainshot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5210cd017ccd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp, masks, way, shot)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mway\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrainshot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# W S 64 21 21 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mdistmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B 21 21 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Do the Thing!\n",
    "\n",
    "start = time.time()\n",
    "trainlosses, testlosses, acctracker = [[] for _ in range(esize)],[[] for _ in range(esize)],[[] for _ in range(esize)]\n",
    "epochs = 5*epoch\n",
    "for e in range(epochs):\n",
    "    # Adjust learnrate\n",
    "    if e%epoch == 0:\n",
    "        [s.step() for s in sscheduler]\n",
    "    \n",
    "    # Train for one epoch\n",
    "    trainloss = train(train_loader, e, gpu, vbity)\n",
    "    \n",
    "    # Evaluate, single pass\n",
    "    testloss, acc, _ = validate(test_loader, gpu, vbity, 1, True)\n",
    "    \n",
    "    # update the precision graph, report\n",
    "    display.clear_output(wait=True)\n",
    "    for j in range(esize):\n",
    "        trainlosses[j].append(trainloss[j])\n",
    "        testlosses[j].append(testloss[j])\n",
    "        acctracker[j].append(acc[j])\n",
    "    pl.figure(1, figsize=(15,15))\n",
    "    for i in range(esize):\n",
    "        pl.subplot(esize,2,2*i+1)\n",
    "        pl.plot(trainlosses[i])\n",
    "        pl.plot(testlosses[i])\n",
    "        pl.ylim((0,2))\n",
    "        pl.title(\"Loss: Training Blue, Validation Gold\")\n",
    "        pl.subplot(esize,2,2*i+2)\n",
    "        pl.plot(acctracker[i][::-1])\n",
    "        pl.ylim((0,1))\n",
    "        pl.title(\"Validation Acc\")\n",
    "    pl.show()\n",
    "    print(\"Train loss is: \"+str(trainloss)+\n",
    "            \"\\nValidation accuracy is: \"+str(acc)+\n",
    "            \"\\nValidation loss is: \"+str(testloss)+\"\\n\")\n",
    "    \n",
    "    print(\"%.2f hours to completion\"%(  (time.time()-start)/(e+1)*(epochs-e)/3600  ))\n",
    "    print()\n",
    "print((time.time()-start)/3600, \"hours total\") \n",
    "_, score, conf = validate(test_loader, gpu, vbity, 10, False)\n",
    "for i in range(esize):\n",
    "    print('Model %d final score: %.2f +- %.2f'%(i, score[i]*100, conf[i]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save([m.cpu().state_dict() for m in smodel], 'detector_TEMP_folded.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
