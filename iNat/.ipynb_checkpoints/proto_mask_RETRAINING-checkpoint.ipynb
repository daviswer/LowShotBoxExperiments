{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "\n",
    "from IPython import display\n",
    "from copy import deepcopy\n",
    "from skimage.transform import resize\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Important Values\n",
    "\n",
    "data = '/data/mini_inat/'\n",
    "gpu = 0\n",
    "torch.cuda.set_device(gpu)\n",
    "workers = 8\n",
    "epoch = 10\n",
    "start_epoch = 0\n",
    "vbity = 20\n",
    "\n",
    "way = 20\n",
    "evalway = 5\n",
    "trainshot = 5\n",
    "testshot = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113664  parameters in neural net.\n"
     ]
    }
   ],
   "source": [
    "# Make Model\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, insize, outsize):\n",
    "        super(Block, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(insize, outsize, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(outsize)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return self.layers(inp)\n",
    "\n",
    "class PROTO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PROTO, self).__init__()\n",
    "        self.process = nn.Sequential(\n",
    "            Block(4,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            Block(64,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            Block(64,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            Block(64,64),\n",
    "            nn.AvgPool2d(10)\n",
    "        )\n",
    "        self.sm = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, inp, way, supshot):\n",
    "        out = self.process(inp[:,:,:,:]).view(inp.size(0),-1)\n",
    "        support = out[:way*supshot].view(way,supshot,-1)\n",
    "        support = torch.mean(support, 1).view(support.size(0),-1)\n",
    "        query = out[way*supshot:]\n",
    "        distmat = torch.sum((support.unsqueeze(0)-query.unsqueeze(1))**2,2).squeeze().neg()\n",
    "        probs = self.sm(distmat)\n",
    "        return probs\n",
    "\n",
    "model = PROTO().cuda(device = gpu)\n",
    "optimizer = optim.Adam(model.parameters(), lr=.001/(2**0))\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda x: 1/(2**x))\n",
    "criterion = nn.NLLLoss().cuda(device = gpu)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "nweights = sum([i.numel() for i in list(model.parameters())])\n",
    "print(nweights,\" parameters in neural net.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackers, m, o = torch.load('saved_models/masks_frozen.torch')\n",
    "model.load_state_dict(m)\n",
    "optimizer.load_state_dict(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load Training/Testing Data\n",
    "\n",
    "d_train = torch.load(data+'train_boxes.pth')\n",
    "d_test = torch.load(data+'val_boxes.pth')\n",
    "d_boxes = torch.load(data+'box_coords.pth')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4905, 0.4961, 0.4330],std=[0.1737, 0.1713, 0.1779])\n",
    "    ])\n",
    "\n",
    "def load_transform(path, pathdict, boxdict, transform, size, flipping, masking):\n",
    "    flip = np.random.choice([True, False])\n",
    "    with open(path, 'rb') as f:\n",
    "        p = Image.open(f)\n",
    "        p = p.convert('RGB')\n",
    "    w,h = p.size\n",
    "    p = p.resize((size, size), Image.BILINEAR)\n",
    "    if flip and flipping:\n",
    "        p = p.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    t = transform(p)\n",
    "    if masking:\n",
    "        mask = np.zeros((h,w))\n",
    "        boxes = pathdict[path]\n",
    "        for b in boxes:\n",
    "            box = boxdict[b]\n",
    "            xmin = box[0]\n",
    "            xmax = box[2]+xmin\n",
    "            ymin = box[1]\n",
    "            ymax = box[3]+ymin\n",
    "            if not flip or not flipping:\n",
    "                mask[ymin:ymax, xmin:xmax] = 1\n",
    "            else:\n",
    "                mask[ymin:ymax, w-xmax:w-xmin] = 1\n",
    "        mask = resize(mask, (size,size), mode='constant', cval=0)\n",
    "        t = torch.cat([t, torch.FloatTensor(mask).unsqueeze(0)], dim=0)\n",
    "    return t\n",
    "\n",
    "class ProtoSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, data_source, way):\n",
    "        iddict = dict()\n",
    "        for i,(_,cat) in enumerate(data_source.imgs):\n",
    "            if cat in iddict:\n",
    "                iddict[cat].append(i)\n",
    "            else:\n",
    "                iddict[cat] = [i]\n",
    "        self.iddict = iddict\n",
    "        self.way = way\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Build new dictionary, shuffle entries\n",
    "        trackdict = deepcopy(self.iddict)\n",
    "        for key in trackdict:\n",
    "            np.random.shuffle(trackdict[key])\n",
    "        # Choose categories, sample, eliminate small categories\n",
    "        idlist = []\n",
    "        while len(trackdict.keys()) >= self.way:\n",
    "            cats = np.random.choice(list(trackdict.keys()), size=self.way, replace=False)\n",
    "            for cat in cats:\n",
    "                for _ in range(trainshot):\n",
    "                    idlist.append(trackdict[cat].pop())\n",
    "            for cat in cats:\n",
    "                for _ in range(testshot):\n",
    "                    idlist.append(trackdict[cat].pop())\n",
    "                if len(trackdict[cat]) < trainshot+testshot:\n",
    "                    trackdict.pop(cat)\n",
    "            yield idlist\n",
    "            idlist = []\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    data+'train', \n",
    "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    data+'val',\n",
    "    loader = lambda x: load_transform(x, d_test, d_boxes, transform, 84, False, True))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_sampler = ProtoSampler(train_dataset, way),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_sampler = ProtoSampler(test_dataset, evalway),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "print('Data loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Define the Procedures\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, gpu, vbity):\n",
    "    model.train()\n",
    "    targ = Variable(torch.LongTensor([i//testshot for i in range(testshot*way)])).cuda(\n",
    "        device = gpu, async=True)\n",
    "    allloss = 0\n",
    "    for i, (inp, _) in enumerate(train_loader):\n",
    "        inp = Variable(inp).cuda(device = gpu, async=True)\n",
    "        \n",
    "        # Zero out the masks\n",
    "#         inp[way*trainshot:,-1,:,:] = 0\n",
    "        \n",
    "        model.zero_grad()\n",
    "        out = model(inp, way, trainshot)\n",
    "        loss = criterion(out, targ)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        allloss += loss.item()\n",
    "        if i%vbity == 0:\n",
    "            print('%d of approx. 192270'%(i*way*(trainshot+testshot)))\n",
    "    return allloss/i\n",
    "\n",
    "def validate(val_loader, model, criterion, gpu, vbity, reps, verbose):\n",
    "    model.eval()\n",
    "    targ = Variable(torch.LongTensor([i//testshot for i in range(testshot*evalway)])).cuda(\n",
    "        device = gpu, async=True)\n",
    "    allloss = 0\n",
    "    acctracker = []\n",
    "    for r in range(reps):\n",
    "        for i, (inp, _) in enumerate(val_loader):\n",
    "            inp = Variable(inp).cuda(device = gpu, async=True)\n",
    "            \n",
    "            # Zero out the masks\n",
    "#             inp[evalway*trainshot:,-1,:,:] = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = model(inp, evalway, trainshot)\n",
    "                loss = criterion(out, targ)\n",
    "                allloss += loss.item()\n",
    "                _,bins = torch.max(out,1)\n",
    "                acc = torch.sum(torch.eq(bins,targ)).item()/testshot/evalway\n",
    "                acctracker.append(acc)\n",
    "            if i%vbity == 0 and verbose:\n",
    "                print('Round %d of %d, %d of approx. 51716'%(r+1, reps, i*evalway*(trainshot+testshot)))\n",
    "    return allloss/i, np.mean(acctracker), 1.96*np.sqrt(np.var(acctracker)/len(acctracker))\n",
    "\n",
    "print('Ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 of 10, 0 of approx. 51716\n",
      "Round 1 of 10, 2000 of approx. 51716\n",
      "Round 1 of 10, 4000 of approx. 51716\n",
      "Round 1 of 10, 6000 of approx. 51716\n",
      "Round 1 of 10, 8000 of approx. 51716\n",
      "Round 1 of 10, 10000 of approx. 51716\n",
      "Round 1 of 10, 12000 of approx. 51716\n",
      "Round 1 of 10, 14000 of approx. 51716\n",
      "Round 1 of 10, 16000 of approx. 51716\n",
      "Round 1 of 10, 18000 of approx. 51716\n",
      "Round 1 of 10, 20000 of approx. 51716\n",
      "Round 1 of 10, 22000 of approx. 51716\n",
      "Round 1 of 10, 24000 of approx. 51716\n",
      "Round 1 of 10, 26000 of approx. 51716\n",
      "Round 1 of 10, 28000 of approx. 51716\n",
      "Round 1 of 10, 30000 of approx. 51716\n",
      "Round 1 of 10, 32000 of approx. 51716\n",
      "Round 1 of 10, 34000 of approx. 51716\n",
      "Round 1 of 10, 36000 of approx. 51716\n",
      "Round 1 of 10, 38000 of approx. 51716\n",
      "Round 1 of 10, 40000 of approx. 51716\n",
      "Round 1 of 10, 42000 of approx. 51716\n",
      "Round 1 of 10, 44000 of approx. 51716\n",
      "Round 1 of 10, 46000 of approx. 51716\n",
      "Round 1 of 10, 48000 of approx. 51716\n",
      "Round 2 of 10, 0 of approx. 51716\n",
      "Round 2 of 10, 2000 of approx. 51716\n",
      "Round 2 of 10, 4000 of approx. 51716\n",
      "Round 2 of 10, 6000 of approx. 51716\n",
      "Round 2 of 10, 8000 of approx. 51716\n",
      "Round 2 of 10, 10000 of approx. 51716\n",
      "Round 2 of 10, 12000 of approx. 51716\n",
      "Round 2 of 10, 14000 of approx. 51716\n",
      "Round 2 of 10, 16000 of approx. 51716\n",
      "Round 2 of 10, 18000 of approx. 51716\n",
      "Round 2 of 10, 20000 of approx. 51716\n",
      "Round 2 of 10, 22000 of approx. 51716\n",
      "Round 2 of 10, 24000 of approx. 51716\n",
      "Round 2 of 10, 26000 of approx. 51716\n",
      "Round 2 of 10, 28000 of approx. 51716\n",
      "Round 2 of 10, 30000 of approx. 51716\n",
      "Round 2 of 10, 32000 of approx. 51716\n",
      "Round 2 of 10, 34000 of approx. 51716\n",
      "Round 2 of 10, 36000 of approx. 51716\n",
      "Round 2 of 10, 38000 of approx. 51716\n",
      "Round 2 of 10, 40000 of approx. 51716\n",
      "Round 2 of 10, 42000 of approx. 51716\n",
      "Round 2 of 10, 44000 of approx. 51716\n",
      "Round 2 of 10, 46000 of approx. 51716\n",
      "Round 2 of 10, 48000 of approx. 51716\n",
      "Round 3 of 10, 0 of approx. 51716\n",
      "Round 3 of 10, 2000 of approx. 51716\n",
      "Round 3 of 10, 4000 of approx. 51716\n",
      "Round 3 of 10, 6000 of approx. 51716\n",
      "Round 3 of 10, 8000 of approx. 51716\n",
      "Round 3 of 10, 10000 of approx. 51716\n",
      "Round 3 of 10, 12000 of approx. 51716\n",
      "Round 3 of 10, 14000 of approx. 51716\n",
      "Round 3 of 10, 16000 of approx. 51716\n",
      "Round 3 of 10, 18000 of approx. 51716\n",
      "Round 3 of 10, 20000 of approx. 51716\n",
      "Round 3 of 10, 22000 of approx. 51716\n",
      "Round 3 of 10, 24000 of approx. 51716\n",
      "Round 3 of 10, 26000 of approx. 51716\n",
      "Round 3 of 10, 28000 of approx. 51716\n",
      "Round 3 of 10, 30000 of approx. 51716\n",
      "Round 3 of 10, 32000 of approx. 51716\n",
      "Round 3 of 10, 34000 of approx. 51716\n",
      "Round 3 of 10, 36000 of approx. 51716\n",
      "Round 3 of 10, 38000 of approx. 51716\n",
      "Round 3 of 10, 40000 of approx. 51716\n",
      "Round 3 of 10, 42000 of approx. 51716\n",
      "Round 3 of 10, 44000 of approx. 51716\n",
      "Round 3 of 10, 46000 of approx. 51716\n",
      "Round 3 of 10, 48000 of approx. 51716\n",
      "Round 4 of 10, 0 of approx. 51716\n",
      "Round 4 of 10, 2000 of approx. 51716\n",
      "Round 4 of 10, 4000 of approx. 51716\n",
      "Round 4 of 10, 6000 of approx. 51716\n",
      "Round 4 of 10, 8000 of approx. 51716\n",
      "Round 4 of 10, 10000 of approx. 51716\n",
      "Round 4 of 10, 12000 of approx. 51716\n",
      "Round 4 of 10, 14000 of approx. 51716\n",
      "Round 4 of 10, 16000 of approx. 51716\n",
      "Round 4 of 10, 18000 of approx. 51716\n",
      "Round 4 of 10, 20000 of approx. 51716\n",
      "Round 4 of 10, 22000 of approx. 51716\n",
      "Round 4 of 10, 24000 of approx. 51716\n",
      "Round 4 of 10, 26000 of approx. 51716\n",
      "Round 4 of 10, 28000 of approx. 51716\n",
      "Round 4 of 10, 30000 of approx. 51716\n",
      "Round 4 of 10, 32000 of approx. 51716\n",
      "Round 4 of 10, 34000 of approx. 51716\n",
      "Round 4 of 10, 36000 of approx. 51716\n",
      "Round 4 of 10, 38000 of approx. 51716\n",
      "Round 4 of 10, 40000 of approx. 51716\n",
      "Round 4 of 10, 42000 of approx. 51716\n",
      "Round 4 of 10, 44000 of approx. 51716\n",
      "Round 4 of 10, 46000 of approx. 51716\n",
      "Round 4 of 10, 48000 of approx. 51716\n",
      "Round 5 of 10, 0 of approx. 51716\n",
      "Round 5 of 10, 2000 of approx. 51716\n",
      "Round 5 of 10, 4000 of approx. 51716\n",
      "Round 5 of 10, 6000 of approx. 51716\n",
      "Round 5 of 10, 8000 of approx. 51716\n",
      "Round 5 of 10, 10000 of approx. 51716\n",
      "Round 5 of 10, 12000 of approx. 51716\n",
      "Round 5 of 10, 14000 of approx. 51716\n",
      "Round 5 of 10, 16000 of approx. 51716\n",
      "Round 5 of 10, 18000 of approx. 51716\n",
      "Round 5 of 10, 20000 of approx. 51716\n",
      "Round 5 of 10, 22000 of approx. 51716\n",
      "Round 5 of 10, 24000 of approx. 51716\n",
      "Round 5 of 10, 26000 of approx. 51716\n",
      "Round 5 of 10, 28000 of approx. 51716\n",
      "Round 5 of 10, 30000 of approx. 51716\n",
      "Round 5 of 10, 32000 of approx. 51716\n",
      "Round 5 of 10, 34000 of approx. 51716\n",
      "Round 5 of 10, 36000 of approx. 51716\n",
      "Round 5 of 10, 38000 of approx. 51716\n",
      "Round 5 of 10, 40000 of approx. 51716\n",
      "Round 5 of 10, 42000 of approx. 51716\n",
      "Round 5 of 10, 44000 of approx. 51716\n",
      "Round 5 of 10, 46000 of approx. 51716\n",
      "Round 5 of 10, 48000 of approx. 51716\n",
      "Round 6 of 10, 0 of approx. 51716\n",
      "Round 6 of 10, 2000 of approx. 51716\n",
      "Round 6 of 10, 4000 of approx. 51716\n",
      "Round 6 of 10, 6000 of approx. 51716\n",
      "Round 6 of 10, 8000 of approx. 51716\n",
      "Round 6 of 10, 10000 of approx. 51716\n",
      "Round 6 of 10, 12000 of approx. 51716\n",
      "Round 6 of 10, 14000 of approx. 51716\n",
      "Round 6 of 10, 16000 of approx. 51716\n",
      "Round 6 of 10, 18000 of approx. 51716\n",
      "Round 6 of 10, 20000 of approx. 51716\n",
      "Round 6 of 10, 22000 of approx. 51716\n",
      "Round 6 of 10, 24000 of approx. 51716\n",
      "Round 6 of 10, 26000 of approx. 51716\n",
      "Round 6 of 10, 28000 of approx. 51716\n",
      "Round 6 of 10, 30000 of approx. 51716\n",
      "Round 6 of 10, 32000 of approx. 51716\n",
      "Round 6 of 10, 34000 of approx. 51716\n",
      "Round 6 of 10, 36000 of approx. 51716\n",
      "Round 6 of 10, 38000 of approx. 51716\n",
      "Round 6 of 10, 40000 of approx. 51716\n",
      "Round 6 of 10, 42000 of approx. 51716\n",
      "Round 6 of 10, 44000 of approx. 51716\n",
      "Round 6 of 10, 46000 of approx. 51716\n",
      "Round 6 of 10, 48000 of approx. 51716\n",
      "Round 7 of 10, 0 of approx. 51716\n",
      "Round 7 of 10, 2000 of approx. 51716\n",
      "Round 7 of 10, 4000 of approx. 51716\n",
      "Round 7 of 10, 6000 of approx. 51716\n",
      "Round 7 of 10, 8000 of approx. 51716\n",
      "Round 7 of 10, 10000 of approx. 51716\n",
      "Round 7 of 10, 12000 of approx. 51716\n",
      "Round 7 of 10, 14000 of approx. 51716\n",
      "Round 7 of 10, 16000 of approx. 51716\n",
      "Round 7 of 10, 18000 of approx. 51716\n",
      "Round 7 of 10, 20000 of approx. 51716\n",
      "Round 7 of 10, 22000 of approx. 51716\n",
      "Round 7 of 10, 24000 of approx. 51716\n",
      "Round 7 of 10, 26000 of approx. 51716\n",
      "Round 7 of 10, 28000 of approx. 51716\n",
      "Round 7 of 10, 30000 of approx. 51716\n",
      "Round 7 of 10, 32000 of approx. 51716\n",
      "Round 7 of 10, 34000 of approx. 51716\n",
      "Round 7 of 10, 36000 of approx. 51716\n",
      "Round 7 of 10, 38000 of approx. 51716\n",
      "Round 7 of 10, 40000 of approx. 51716\n",
      "Round 7 of 10, 42000 of approx. 51716\n",
      "Round 7 of 10, 44000 of approx. 51716\n",
      "Round 7 of 10, 46000 of approx. 51716\n",
      "Round 7 of 10, 48000 of approx. 51716\n",
      "Round 8 of 10, 0 of approx. 51716\n",
      "Round 8 of 10, 2000 of approx. 51716\n",
      "Round 8 of 10, 4000 of approx. 51716\n",
      "Round 8 of 10, 6000 of approx. 51716\n",
      "Round 8 of 10, 8000 of approx. 51716\n",
      "Round 8 of 10, 10000 of approx. 51716\n",
      "Round 8 of 10, 12000 of approx. 51716\n",
      "Round 8 of 10, 14000 of approx. 51716\n",
      "Round 8 of 10, 16000 of approx. 51716\n",
      "Round 8 of 10, 18000 of approx. 51716\n",
      "Round 8 of 10, 20000 of approx. 51716\n",
      "Round 8 of 10, 22000 of approx. 51716\n",
      "Round 8 of 10, 24000 of approx. 51716\n",
      "Round 8 of 10, 26000 of approx. 51716\n",
      "Round 8 of 10, 28000 of approx. 51716\n",
      "Round 8 of 10, 30000 of approx. 51716\n",
      "Round 8 of 10, 32000 of approx. 51716\n",
      "Round 8 of 10, 34000 of approx. 51716\n",
      "Round 8 of 10, 36000 of approx. 51716\n",
      "Round 8 of 10, 38000 of approx. 51716\n",
      "Round 8 of 10, 40000 of approx. 51716\n",
      "Round 8 of 10, 42000 of approx. 51716\n",
      "Round 8 of 10, 44000 of approx. 51716\n",
      "Round 8 of 10, 46000 of approx. 51716\n",
      "Round 8 of 10, 48000 of approx. 51716\n",
      "Round 9 of 10, 0 of approx. 51716\n",
      "Round 9 of 10, 2000 of approx. 51716\n",
      "Round 9 of 10, 4000 of approx. 51716\n",
      "Round 9 of 10, 6000 of approx. 51716\n",
      "Round 9 of 10, 8000 of approx. 51716\n",
      "Round 9 of 10, 10000 of approx. 51716\n",
      "Round 9 of 10, 12000 of approx. 51716\n",
      "Round 9 of 10, 14000 of approx. 51716\n",
      "Round 9 of 10, 16000 of approx. 51716\n",
      "Round 9 of 10, 18000 of approx. 51716\n",
      "Round 9 of 10, 20000 of approx. 51716\n",
      "Round 9 of 10, 22000 of approx. 51716\n",
      "Round 9 of 10, 24000 of approx. 51716\n",
      "Round 9 of 10, 26000 of approx. 51716\n",
      "Round 9 of 10, 28000 of approx. 51716\n",
      "Round 9 of 10, 30000 of approx. 51716\n",
      "Round 9 of 10, 32000 of approx. 51716\n",
      "Round 9 of 10, 34000 of approx. 51716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 9 of 10, 36000 of approx. 51716\n",
      "Round 9 of 10, 38000 of approx. 51716\n",
      "Round 9 of 10, 40000 of approx. 51716\n",
      "Round 9 of 10, 42000 of approx. 51716\n",
      "Round 9 of 10, 44000 of approx. 51716\n",
      "Round 9 of 10, 46000 of approx. 51716\n",
      "Round 9 of 10, 48000 of approx. 51716\n",
      "Round 10 of 10, 0 of approx. 51716\n",
      "Round 10 of 10, 2000 of approx. 51716\n",
      "Round 10 of 10, 4000 of approx. 51716\n",
      "Round 10 of 10, 6000 of approx. 51716\n",
      "Round 10 of 10, 8000 of approx. 51716\n",
      "Round 10 of 10, 10000 of approx. 51716\n",
      "Round 10 of 10, 12000 of approx. 51716\n",
      "Round 10 of 10, 14000 of approx. 51716\n",
      "Round 10 of 10, 16000 of approx. 51716\n",
      "Round 10 of 10, 18000 of approx. 51716\n",
      "Round 10 of 10, 20000 of approx. 51716\n",
      "Round 10 of 10, 22000 of approx. 51716\n",
      "Round 10 of 10, 24000 of approx. 51716\n",
      "Round 10 of 10, 26000 of approx. 51716\n",
      "Round 10 of 10, 28000 of approx. 51716\n",
      "Round 10 of 10, 30000 of approx. 51716\n",
      "Round 10 of 10, 32000 of approx. 51716\n",
      "Round 10 of 10, 34000 of approx. 51716\n",
      "Round 10 of 10, 36000 of approx. 51716\n",
      "Round 10 of 10, 38000 of approx. 51716\n",
      "Round 10 of 10, 40000 of approx. 51716\n",
      "Round 10 of 10, 42000 of approx. 51716\n",
      "Round 10 of 10, 44000 of approx. 51716\n",
      "Round 10 of 10, 46000 of approx. 51716\n",
      "Round 10 of 10, 48000 of approx. 51716\n",
      "Final score: 47.55 +- 0.27\n"
     ]
    }
   ],
   "source": [
    "_, score, conf = validate(test_loader, model, criterion, gpu, vbity, 10, True)\n",
    "print('Final score: %.2f +- %.2f'%(score*100, conf*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of approx. 192270\n",
      "8000 of approx. 192270\n",
      "16000 of approx. 192270\n",
      "24000 of approx. 192270\n",
      "32000 of approx. 192270\n",
      "40000 of approx. 192270\n",
      "48000 of approx. 192270\n",
      "56000 of approx. 192270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-6:\n",
      "Process Process-3:\n",
      "Process Process-1:\n",
      "Process Process-4:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-7:\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-5:\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 16, in load_transform\n",
      "    p = p.convert('RGB')\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "Process Process-8:\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 14, in load_transform\n",
      "    with open(path, 'rb') as f:\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "KeyboardInterrupt\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 16, in load_transform\n",
      "    p = p.convert('RGB')\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 21, in load_transform\n",
      "    t = transform(p)\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 16, in load_transform\n",
      "    p = p.convert('RGB')\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 42, in __call__\n",
      "    img = t(img)\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 15, in load_transform\n",
      "    p = Image.open(f)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 69, in __call__\n",
      "    return F.to_tensor(pic)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 2557, in open\n",
      "    prefix = fp.read(16)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "KeyboardInterrupt\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 63, in to_tensor\n",
      "    img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 16, in load_transform\n",
      "    p = p.convert('RGB')\n",
      "KeyboardInterrupt\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 215, in load\n",
      "    s = read(self.decodermaxblock)\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/JpegImagePlugin.py\", line 362, in load_read\n",
      "    s = self.fp.read(read_bytes)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 16, in load_transform\n",
      "    p = p.convert('RGB')\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 215, in load\n",
      "    s = read(self.decodermaxblock)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/JpegImagePlugin.py\", line 362, in load_read\n",
      "    s = self.fp.read(read_bytes)\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-4c67d3a77dc3>\", line 12, in <module>\n",
      "    trainloss = train(train_loader, model, criterion, optimizer, e, gpu, vbity)\n",
      "  File \"<ipython-input-6-b9015b46ae33>\", line 8, in train\n",
      "    for i, (inp, _) in enumerate(train_loader):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 329, in __next__\n",
      "    idx, batch = self._get_batch()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 308, in _get_batch\n",
      "    return self.data_queue.get()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/queue.py\", line 164, in get\n",
      "    self.not_empty.wait()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/threading.py\", line 295, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/posixpath.py\", line 389, in realpath\n",
      "    return abspath(path)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/posixpath.py\", line 372, in abspath\n",
      "    if not isabs(path):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/posixpath.py\", line 67, in isabs\n",
      "    sep = _get_sep(s)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/posixpath.py\", line 42, in _get_sep\n",
      "    if isinstance(path, bytes):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 25429) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Do the Thing!\n",
    "\n",
    "start = time.time()\n",
    "trainlosses, testlosses, acctracker, spreadtracker = trackers #[], [], [], []\n",
    "epochs = 5*epoch\n",
    "for e in range(epochs):\n",
    "    # Adjust learnrate\n",
    "    if e%epoch == 0 and e>0:\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Train for one epoch\n",
    "    trainloss = train(train_loader, model, criterion, optimizer, e, gpu, vbity)\n",
    "    \n",
    "    # Evaluate, single pass\n",
    "    testloss, acc, spread = validate(test_loader, model, criterion, gpu, vbity, 1, True)\n",
    "    \n",
    "    # update the precision graph, report\n",
    "    display.clear_output(wait=True)\n",
    "    trainlosses.append(trainloss)\n",
    "    testlosses.append(testloss)\n",
    "    acctracker.append(acc)\n",
    "    spreadtracker.append(spread)\n",
    "    pl.figure(1, figsize=(15,5))\n",
    "    pl.subplot(1,2,1)\n",
    "    pl.plot(trainlosses)\n",
    "    pl.plot(testlosses)\n",
    "    pl.axvline(x=50)\n",
    "    pl.ylim((.5,3))\n",
    "    pl.title(\"Loss: Training Blue, Validation Gold\")\n",
    "    pl.subplot(1,2,2)\n",
    "    pl.plot(acctracker[::-1])\n",
    "    pl.plot(np.array(acctracker[::-1])-np.array(spreadtracker[::-1]), alpha=.5, c='blue')\n",
    "    pl.plot(np.array(acctracker[::-1])+np.array(spreadtracker[::-1]), alpha=.5, c='blue')\n",
    "    pl.axvline(x=50)\n",
    "    pl.ylim((0.3,.8))\n",
    "    pl.title(\"Validation Acc\")\n",
    "    pl.show()\n",
    "    print(\"Train loss is: \"+str(trainloss)+\n",
    "            \"\\nValidation accuracy is: \"+str(acc)+\n",
    "            \"\\nValidation loss is: \"+str(testloss)+\"\\n\")\n",
    "    \n",
    "    print(\"%.2f hours to completion\"%(  (time.time()-start)/(e+1)*(epochs-e)/3600  ))\n",
    "    print()\n",
    "print((time.time()-start)/3600) \n",
    "_, score, conf = validate(test_loader, model, criterion, gpu, vbity, 10, False)\n",
    "print('Final score: %.2f +- %.2f'%(score*100, conf*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-682a2aafde74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'baselinemodel.torch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'baselinemodel.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.session.delete();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed,model,losstracker,evallosstracker,evalacctracker = torch.load('saved_models/naive_4.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
