{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "\n",
    "from IPython import display\n",
    "from copy import deepcopy\n",
    "from skimage.transform import resize\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Important Values\n",
    "\n",
    "data = '/data/mini_inat/'\n",
    "gpu = 0\n",
    "torch.cuda.set_device(gpu)\n",
    "workers = 8\n",
    "epoch = 10\n",
    "start_epoch = 0\n",
    "vbity = 20\n",
    "\n",
    "way = 20\n",
    "evalway = 5\n",
    "trainshot = 5\n",
    "testshot = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113664  parameters in each neural net.\n"
     ]
    }
   ],
   "source": [
    "# Make Model\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, insize, outsize):\n",
    "        super(Block, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(insize, outsize, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(outsize)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return self.layers(inp)\n",
    "\n",
    "class PROTO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PROTO, self).__init__()\n",
    "        self.process = nn.Sequential(\n",
    "            Block(4,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            Block(64,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            Block(64,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            Block(64,64),\n",
    "            nn.AvgPool2d(10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        return self.process(inp).view(inp.size(0),-1)\n",
    "        \n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.sm = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, support, query, way):\n",
    "        support = support.view(way,-1,support.size(1)).mean(1).squeeze()\n",
    "        distmat = torch.sum((support.unsqueeze(0)-query.unsqueeze(1))**2,2).squeeze().neg()\n",
    "        probs = self.sm(distmat)\n",
    "        return probs\n",
    "\n",
    "smodel = PROTO().cuda(device = gpu)\n",
    "qmodel = PROTO().cuda(device = gpu)\n",
    "predictor = Predictor().cuda(device = gpu)\n",
    "soptimizer = optim.Adam(smodel.parameters(), lr=.001/(2**0))\n",
    "qoptimizer = optim.Adam(qmodel.parameters(), lr=.001/(2**0))\n",
    "sscheduler = optim.lr_scheduler.LambdaLR(soptimizer, lambda x: 1/(2**x))\n",
    "qscheduler = optim.lr_scheduler.LambdaLR(qoptimizer, lambda x: 1/(2**x))\n",
    "criterion = nn.NLLLoss().cuda(device = gpu)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "nweights = sum([i.numel() for i in list(smodel.parameters())])\n",
    "print(nweights,\" parameters in each neural net.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackers, m, o = torch.load('saved_models/masks_frozen.torch')\n",
    "smodel.load_state_dict(m)\n",
    "qmodel.load_state_dict(m)\n",
    "soptimizer.load_state_dict(o)\n",
    "qoptimizer.load_state_dict(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load Training/Testing Data\n",
    "\n",
    "d_train = torch.load(data+'train_boxes.pth')\n",
    "d_test = torch.load(data+'val_boxes.pth')\n",
    "d_boxes = torch.load(data+'box_coords.pth')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4905, 0.4961, 0.4330],std=[0.1737, 0.1713, 0.1779])\n",
    "    ])\n",
    "\n",
    "def load_transform(path, pathdict, boxdict, transform, size, flipping, masking):\n",
    "    flip = np.random.choice([True, False])\n",
    "    with open(path, 'rb') as f:\n",
    "        p = Image.open(f)\n",
    "        p = p.convert('RGB')\n",
    "    w,h = p.size\n",
    "    p = p.resize((size, size), Image.BILINEAR)\n",
    "    if flip and flipping:\n",
    "        p = p.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    t = transform(p)\n",
    "    if masking:\n",
    "        mask = np.zeros((h,w))\n",
    "        boxes = pathdict[path]\n",
    "        for b in boxes:\n",
    "            box = boxdict[b]\n",
    "            xmin = box[0]\n",
    "            xmax = box[2]+xmin\n",
    "            ymin = box[1]\n",
    "            ymax = box[3]+ymin\n",
    "            if not flip or not flipping:\n",
    "                mask[ymin:ymax, xmin:xmax] = 1\n",
    "            else:\n",
    "                mask[ymin:ymax, w-xmax:w-xmin] = 1\n",
    "        mask = resize(mask, (size,size), mode='constant', cval=0)\n",
    "        t = torch.cat([t, torch.FloatTensor(mask).unsqueeze(0)], dim=0)\n",
    "    return t\n",
    "\n",
    "class ProtoSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, data_source, way):\n",
    "        iddict = dict()\n",
    "        for i,(_,cat) in enumerate(data_source.imgs):\n",
    "            if cat in iddict:\n",
    "                iddict[cat].append(i)\n",
    "            else:\n",
    "                iddict[cat] = [i]\n",
    "        self.iddict = iddict\n",
    "        self.way = way\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Build new dictionary, shuffle entries\n",
    "        trackdict = deepcopy(self.iddict)\n",
    "        for key in trackdict:\n",
    "            np.random.shuffle(trackdict[key])\n",
    "        # Choose categories, sample, eliminate small categories\n",
    "        idlist = []\n",
    "        while len(trackdict.keys()) >= self.way:\n",
    "            cats = np.random.choice(list(trackdict.keys()), size=self.way, replace=False)\n",
    "            for cat in cats:\n",
    "                for _ in range(trainshot):\n",
    "                    idlist.append(trackdict[cat].pop())\n",
    "            for cat in cats:\n",
    "                for _ in range(testshot):\n",
    "                    idlist.append(trackdict[cat].pop())\n",
    "                if len(trackdict[cat]) < trainshot+testshot:\n",
    "                    trackdict.pop(cat)\n",
    "            yield idlist\n",
    "            idlist = []\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    data+'train', \n",
    "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    data+'val',\n",
    "    loader = lambda x: load_transform(x, d_test, d_boxes, transform, 84, False, True))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_sampler = ProtoSampler(train_dataset, way),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_sampler = ProtoSampler(test_dataset, evalway),\n",
    "    num_workers = workers,\n",
    "    pin_memory = True)\n",
    "print('Data loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Define the Procedures\n",
    "\n",
    "def train(train_loader, epoch, gpu, vbity):\n",
    "    smodel.train()\n",
    "    qmodel.train()\n",
    "    targ = Variable(torch.LongTensor([i//testshot for i in range(testshot*way)])).cuda(\n",
    "        device = gpu, async=True)\n",
    "    allloss = 0\n",
    "    for i, (inp, _) in enumerate(train_loader):\n",
    "        inp = Variable(inp).cuda(device = gpu, async=True)\n",
    "        \n",
    "        # Zero out the masks\n",
    "        inp[way*trainshot:,-1,:,:] = 0\n",
    "        \n",
    "        smodel.zero_grad()\n",
    "        qmodel.zero_grad()\n",
    "        out = predictor(smodel(inp[:way*trainshot]), qmodel(inp[way*trainshot:]), way)\n",
    "        loss = criterion(out, targ)\n",
    "        loss.backward()\n",
    "        soptimizer.step()\n",
    "        qoptimizer.step()\n",
    "        \n",
    "        allloss += loss.item()\n",
    "        if i%vbity == 0:\n",
    "            print('%d of approx. 192270'%(i*way*(trainshot+testshot)))\n",
    "    return allloss/i\n",
    "\n",
    "def validate(val_loader, gpu, vbity, reps, verbose):\n",
    "    smodel.eval()\n",
    "    qmodel.eval()\n",
    "    targ = Variable(torch.LongTensor([i//testshot for i in range(testshot*evalway)])).cuda(\n",
    "        device = gpu, async=True)\n",
    "    allloss = 0\n",
    "    acctracker = []\n",
    "    for r in range(reps):\n",
    "        for i, (inp, _) in enumerate(val_loader):\n",
    "            inp = Variable(inp).cuda(device = gpu, async=True)\n",
    "            \n",
    "            # Zero out the masks\n",
    "            inp[evalway*trainshot:,-1,:,:] = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = predictor(smodel(inp[:evalway*trainshot]), qmodel(inp[evalway*trainshot:]), evalway)\n",
    "                loss = criterion(out, targ)\n",
    "                allloss += loss.item()\n",
    "                _,bins = torch.max(out,1)\n",
    "                acc = torch.sum(torch.eq(bins,targ)).item()/testshot/evalway\n",
    "                acctracker.append(acc)\n",
    "            if i%vbity == 0 and verbose:\n",
    "                print('Round %d of %d, %d of approx. 51716'%(r+1, reps, i*evalway*(trainshot+testshot)))\n",
    "    return allloss/i, np.mean(acctracker), 1.96*np.sqrt(np.var(acctracker)/len(acctracker))\n",
    "\n",
    "print('Ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of approx. 192270\n",
      "8000 of approx. 192270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-8:\n",
      "Process Process-4:\n",
      "Process Process-3:\n",
      "Process Process-6:\n",
      "Process Process-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-5:\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-1:\n",
      "Process Process-2:\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 15, in load_transform\n",
      "    p = Image.open(f)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 2577, in open\n",
      "    im = _open_core(fp, filename, prefix)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 2567, in _open_core\n",
      "    im = factory(fp, filename)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 35, in load_transform\n",
      "    mask = resize(mask, (size,size), mode='constant', cval=0)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/JpegImagePlugin.py\", line 779, in jpeg_factory\n",
      "    im = JpegImageFile(fp, filename)\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 16, in load_transform\n",
      "    p = p.convert('RGB')\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 131, in resize\n",
      "    tform.estimate(src_corners, dst_corners)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 16, in load_transform\n",
      "    p = p.convert('RGB')\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 16, in load_transform\n",
      "    p = p.convert('RGB')\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/skimage/transform/_geometric.py\", line 650, in estimate\n",
      "    dst_matrix, dst = _center_and_normalize_points(dst)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/skimage/transform/_geometric.py\", line 61, in _center_and_normalize_points\n",
      "    pointsh = np.row_stack([points.T, np.ones((points.shape[0]),)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 16, in load_transform\n",
      "    p = p.convert('RGB')\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 215, in load\n",
      "    s = read(self.decodermaxblock)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 102, in __init__\n",
      "    self._open()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torchvision/datasets/folder.py\", line 122, in __getitem__\n",
      "    img = self.loader(path)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/JpegImagePlugin.py\", line 362, in load_read\n",
      "    s = self.fp.read(read_bytes)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 35, in load_transform\n",
      "    mask = resize(mask, (size,size), mode='constant', cval=0)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/JpegImagePlugin.py\", line 306, in _open\n",
      "    s = self.fp.read(1)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 72, in <lambda>\n",
      "    loader = lambda x: load_transform(x, d_train, d_boxes, transform, 84, True, True))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 131, in resize\n",
      "    tform.estimate(src_corners, dst_corners)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/skimage/transform/_geometric.py\", line 679, in estimate\n",
      "    _, _, V = np.linalg.svd(A)\n",
      "  File \"<ipython-input-5-c9918ec3295e>\", line 16, in load_transform\n",
      "    p = p.convert('RGB')\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py\", line 1444, in svd\n",
      "    u, s, vh = gufunc(a, signature=signature, extobj=extobj)\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-9dfab01c89a4>\", line 13, in <module>\n",
      "    trainloss = train(train_loader, e, gpu, vbity)\n",
      "  File \"<ipython-input-6-d72962972395>\", line 9, in train\n",
      "    for i, (inp, _) in enumerate(train_loader):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 329, in __next__\n",
      "    idx, batch = self._get_batch()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 308, in _get_batch\n",
      "    return self.data_queue.get()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/queue.py\", line 164, in get\n",
      "    self.not_empty.wait()\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/threading.py\", line 295, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/inspect.py\", line 709, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/posixpath.py\", line 378, in abspath\n",
      "    return normpath(path)\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/posixpath.py\", line 357, in normpath\n",
      "    if (comp != dotdot or (not initial_slashes and not new_comps) or\n",
      "  File \"/home/dww78/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 28758) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Do the Thing!\n",
    "\n",
    "start = time.time()\n",
    "trainlosses, testlosses, acctracker, spreadtracker = trackers #[], [], [], []\n",
    "epochs = 5*epoch\n",
    "for e in range(epochs):\n",
    "    # Adjust learnrate\n",
    "    if e%epoch == 0:\n",
    "        sscheduler.step()\n",
    "        qscheduler.step()\n",
    "    \n",
    "    # Train for one epoch\n",
    "    trainloss = train(train_loader, e, gpu, vbity)\n",
    "    \n",
    "    # Evaluate, single pass\n",
    "    testloss, acc, spread = validate(test_loader, gpu, vbity, 1, True)\n",
    "    \n",
    "    # update the precision graph, report\n",
    "    display.clear_output(wait=True)\n",
    "    trainlosses.append(trainloss)\n",
    "    testlosses.append(testloss)\n",
    "    acctracker.append(acc)\n",
    "    spreadtracker.append(spread)\n",
    "    pl.figure(1, figsize=(15,5))\n",
    "    pl.subplot(1,2,1)\n",
    "    pl.plot(trainlosses)\n",
    "    pl.plot(testlosses)\n",
    "    pl.axvline(x=49)\n",
    "    pl.ylim((.5,3))\n",
    "    pl.title(\"Loss: Training Blue, Validation Gold\")\n",
    "    pl.subplot(1,2,2)\n",
    "    pl.plot(acctracker[::-1])\n",
    "    pl.plot(np.array(acctracker[::-1])-np.array(spreadtracker[::-1]), alpha=.5, c='blue')\n",
    "    pl.plot(np.array(acctracker[::-1])+np.array(spreadtracker[::-1]), alpha=.5, c='blue')\n",
    "    pl.axvline(x=len(trainlosses)-50)\n",
    "    pl.ylim((0.3,.8))\n",
    "    pl.title(\"Validation Acc\")\n",
    "    pl.show()\n",
    "    print(\"Train loss is: \"+str(trainloss)+\n",
    "            \"\\nValidation accuracy is: \"+str(acc)+\n",
    "            \"\\nValidation loss is: \"+str(testloss)+\"\\n\")\n",
    "    \n",
    "    print(\"%.2f hours to completion\"%(  (time.time()-start)/(e+1)*(epochs-e)/3600  ))\n",
    "    print()\n",
    "print((time.time()-start)/3600) \n",
    "_, score, conf = validate(test_loader, gpu, vbity, 10, False)\n",
    "print('Final score: %.2f +- %.2f'%(score*100, conf*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-682a2aafde74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'baselinemodel.torch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'baselinemodel.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.session.delete();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed,model,losstracker,evallosstracker,evalacctracker = torch.load('saved_models/naive_4.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
